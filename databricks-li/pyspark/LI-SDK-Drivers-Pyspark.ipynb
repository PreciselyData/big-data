{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "885de781-11b5-4ba7-aab7-a930f2e7f25c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**This notebook describes the Classes and APIs used in Location Intelligence SDK for Big Data.**\n",
    "\n",
    "**Supported spatial operations:**\n",
    "\n",
    "**1. PointInPolygon\n",
    "2. SearchNearest\n",
    "3. JoinByDistance\n",
    "4. GenerateHexagon**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f10d13d4-a39a-458e-804b-38eb679c8fe7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "      .master(\"yarn\") \\\n",
    "      .appName(\"Li-sdk-pyspark\") \\\n",
    "      .config(\"spark.sql.legacy.allowUntypedScalaUDF\", True) \\\n",
    "      .getOrCreate();\n",
    "spark.sparkContext.addPyFile('/location_intelligence_bigdata_li_sdk_pyspark_0_SNAPSHOT.zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "54074d68-0d3c-4dc0-af93-c3ac1d02e652",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## **PointInPolygon**\n",
    "\n",
    "**A PointInPolygon Operation: This method filters the point coordinates in input dataframe which are within a specified polygon. (for example, the polygon of the continental USA). Adds output fields from polygon table to input dataset as columns.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "25490a43-7472-4bfc-8ce2-e30bc3594a6e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# perform point in polygon operation\n",
    "from li.SpatialAPI import SpatialAPI\n",
    "fabricPath = \"addressFabric50.csv\" #The HDFS path to input file\n",
    "fabricDF = spark.read.csv(fabricPath, header=True, sep = ',' )\n",
    "pointInPolygonDF = SpatialAPI.pointInPolygon(\n",
    "    inputDF = fabricDF, #dataframe of input dataset\n",
    "    tableFileType =\"TAB\", #Type of spatial data provided\n",
    "    tableFilePath = \"\", #The HDFS path to spatial table\n",
    "    tableFileName = \"\", #Spatial table file name\n",
    "    libraries = None, # libraries in case of geodatabase tableFileType, defaults to None.\n",
    "    longitude = \"lon\", #Longitude column name \n",
    "    latitude = \"lat\", #Latitude column name\n",
    "    includeEmptySearchResults = True, # if true then an empty search will keep the original input row and the new columns will be null and if false then an empty search will result in the row not appearing in the outputted DataFrame\n",
    "    outputFields = [\"ZIP\", \"Name\"] # Fields from the polygon table to include in the output\n",
    ")\n",
    "pointInPolygonDF.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d5a74c15-aa29-426f-a703-79d433beaeb6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## **SearchNearest**\n",
    "**A SearchNearest Operation: This method takes in a geometry string (either in GeoJSON, WKT, KML or WKB format) and searches for it in a table of geometries within a specified distance. Searched geometries counts can be limited by defining maxCandidates parameter. By default, geometries are listed from nearest to farthest.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c8e54bcc-5357-45c0-a904-a84c441997a7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from li.SpatialAPI import SpatialAPI\n",
    "inputPath = \"/FileStore/lisdk/input/geometryGeoJson6.csv\" #The HDFS path to input file\n",
    "inputDF = spark.read.csv(inputPath, header=True, sep = ',' )\n",
    "searchNearestDF = SpatialAPI.searchNearest(\n",
    "        inputDF = inputDF, \n",
    "        tableFileType = \"TAB\", #Type of spatial data provided\n",
    "        tableFilePath = \"\", #The HDFS path to spatial table\n",
    "        tableFileName = \"LANDMARKS.TAB\", #Spatial table file name\n",
    "        maxCandidates = 2, # Maximum number of candidates\n",
    "        distanceValue = 100.0, # The absolute value of buffer length around point 1 to search for point 2\n",
    "        libraries = None,\n",
    "        distanceUnit = \"mi\", # unit of measurement for distanceUnit parameter\n",
    "        distanceColumnName = \"distance\", # Distance column name in output\n",
    "        geometryStringType = \"GeoJSON\", # Type of geometry string data provided,\n",
    "        geometryColumnName = \"geometry\", # Geometry column name for input data\n",
    "        includeEmptySearchResults = True, # if true then an empty search will keep the original input row and the new columns will be null and if false then an empty search will result in the row not appearing in the outputted DataFrame\n",
    "        outputFields = [\"Name\", \"State\", \"Landmark\"], # Fields from the polygon table to include in the output\n",
    ")\n",
    "searchNearestDF.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d9b99709-60cc-42fe-8921-432fe72b2a2a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## **JoinByDistance**\n",
    "**A JoinByDistance Operation: This method joins two dataframes taking longitude and latitude values, one set from each dataframe, representing the location of the record to be joined. The coordinate values must be in CoordSysConstants.longLatWGS84 coordinate system. This method also takes a searchRadius, which is the buffer around the first point to search for the second point to be inside. The last parameter is a geohash precision that will be used within the calculation.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f6d9e43-72c7-4006-936c-466eb4b568be",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from li.DistanceJoinOption import DistanceJoinOption\n",
    "from li.LimitMethods import LimitMethods\n",
    "from li.SpatialAPI import SpatialAPI\n",
    "poiCSV = \"\" #The path to the first input file\n",
    "addressFabricCSV = \"\" # The path to the second input file\n",
    "distanceColumnName = \"outputDistance\" # The output distance column name\n",
    "limitMethod = LimitMethods.RowNumber # The limit method name\n",
    "limitMatches = 7 # Limit Value as the DistanceJoinOption\n",
    "df1 = spark.read.csv(poiCSV, header=True, sep = ',', inferSchema=True)\n",
    "df2 = spark.read.csv(addressFabricCSV, header=True, sep = ',', inferSchema=True)\n",
    "\n",
    "# Perform join by distance\n",
    "joinedDF = SpatialAPI.joinByDistance(df1 = df1,\n",
    "      df2 = df2,\n",
    "      df1Longitude = \"LONGITUDE\", # The longitude column name of the first dataframe\n",
    "      df1Latitude = \"LATITUDE\", # The latitude column name of the first dataframe\n",
    "      df2Longitude = \"LON\", # The longitude column name of the second dataframe\n",
    "      df2Latitude = \"LAT\", # The latitude column name of the second dataframe\n",
    "      searchRadius = 0.5, # The absolute value of buffer length around point 1 to search for point 2\n",
    "      distanceUnit = \"mi\", # unit of measurement for distanceUnit parameter\n",
    "      geoHashPrecision = 7, # The geohash precision\n",
    "      options = {DistanceJoinOption.DistanceColumnName: distanceColumnName, DistanceJoinOption.LimitMatches: limitMatches, DistanceJoinOption.LimitMethod: LimitMethods.RowNumber})\n",
    "joinedDF.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b8d58f22-17b9-4301-9664-25830b570ef1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## **Hexgen**\n",
    "**A HexagonGeneration Operation: This method generates the hexagons within a bounding box defined by minimum and maximum value of longitude and latitude Hexagon output can be used for map display.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa7b93ba-1aaf-4834-9f6e-6678ba626d6b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Perform Hexgen\n",
    "from li.SpatialAPI import SpatialAPI\n",
    "hexGenDF = SpatialAPI.generateHexagon(\n",
    "    sparkSession = spark,\n",
    "    minLongitude = -73.728200, # The bottom left longitude of the bounding box\n",
    "    minLatitude = 40.979800, # The bottom left latitude of the bounding box\n",
    "    maxLongitude = -71.787480, # The upper right longitude of the bounding box\n",
    "    maxLatitude = 42.050496, # The upper right latitude of the bounding box\n",
    "    hexLevel =  3, # The level to generate hexagons for. Must be between 1 and 11\n",
    "    containerLevel = 2, # A hint for providing some parallel hexagon generation. Must be less than the hexLevel property\n",
    "    numOfPartitions = 1, # Number of partitions,\n",
    "    maximumNumOfRowsPerPartition = 5 # Max number of rows per partition. This number will depend on available memory for executor.\n",
    ")\n",
    "hexGenDF.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a31c6cea-caf1-4b1f-9746-bab0a52b86e7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "LI-SDK-Drivers-Pyspark",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
