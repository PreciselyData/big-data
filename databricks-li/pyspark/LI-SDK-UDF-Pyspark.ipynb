{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ee688dd2-a435-4c56-81aa-91a0b20e7cfb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "      .master(\"yarn\") \\\n",
    "      .appName(\"pyspark-udf-li-sdk\") \\\n",
    "      .config(\"spark.sql.legacy.allowUntypedScalaUDF\", True) \\\n",
    "      .getOrCreate();\n",
    "spark.sparkContext.addPyFile('/location_intelligence_bigdata_li_sdk_pyspark_5.2.1.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "480fe168-9e89-4729-bfd6-43c7b22e679b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Importing the Required Classes\n",
    "from li.SQLRegistrator import SQLRegistrator\n",
    "# Register the LI UDFs\n",
    "SQLRegistrator.registerAll()\n",
    "\n",
    "# input file path\n",
    "poly_file_path = \"/STATES.csv\"\n",
    "df = spark.read.csv(poly_file_path, header=True, inferSchema=True)\n",
    "\n",
    "# Create the TemporaryTable from input dataframe\n",
    "df.createOrReplaceTempView(\"polygontest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8cedde2c-fba6-4799-87da-c6b61b813ea2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## **Constructor and Persistence Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "00c2f56c-a9e8-4167-9735-73154bdd4d95",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Geometry from WKT\n",
    "geoFromWKT = spark.sql(\"SELECT ST_GeomFromWKT(WKT) as Geometry, State_Name, State FROM polygontest\")\n",
    "\n",
    "# Geometry from GeoJSON\n",
    "GeoFromGeoJSON = spark.sql(\"SELECT ST_GeomFromGeoJSON(ST_ToGeoJSON(ST_GeomFromWKT(WKT))) as Geometry, State_Name, State FROM polygontest\")\n",
    "\n",
    "# Geometry from WKB\n",
    "GeoFromWKB = spark.sql(\"SELECT ST_GeomFromWKB(ST_ToWKB(ST_GeomFromWKT(WKT))) as Geometry, State_Name, State FROM polygontest\")\n",
    "\n",
    "# Geometry from KML\n",
    "GeoFromKML = spark.sql(\"SELECT ST_GeomFromKML(ST_ToKML(ST_GeomFromWKT(WKT))) as Geometry, State_Name, State FROM polygontest\")\n",
    "\n",
    "# Geometry from Point\n",
    "GeoFromPoint = spark.sql(\"SELECT ST_Point(-73.750333 , 42.736103) as Geometry\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f0ae8eba-7cb2-4127-be82-ddb67fde7dcb",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## **Predicate Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6456b066-74ec-44a5-88f0-376fea68e551",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Prepare two views\n",
    "df1, df2 = df.randomSplit([0.5, 0.5])\n",
    "df1.createOrReplaceTempView(\"geometry1\")\n",
    "df2.createOrReplaceTempView(\"geometry2\")\n",
    "\n",
    "# Check Disjoint\n",
    "disjoint = spark.sql(\"SELECT ST_Disjoint(ST_GeomFromWKT(t1.WKT), ST_GeomFromWKT(t2.WKT)) as IsDisjoint FROM geometry1 t1, geometry2 t2\");\n",
    "\n",
    "# Check Intersects\n",
    "intersect = spark.sql(\"SELECT ST_Intersects(ST_GeomFromWKT(t1.WKT), ST_GeomFromWKT(t2.WKT)) as Intersects FROM geometry1 t1, geometry2 t2\");\n",
    "\n",
    "# Check Overlap\n",
    "overlap = spark.sql(\"SELECT ST_Overlaps(ST_GeomFromWKT(t1.WKT), ST_GeomFromWKT(t2.WKT)) as Overlap FROM geometry1 t1, geometry2 t2\");\n",
    "\n",
    "# Check within\n",
    "within = spark.sql(\"SELECT ST_Within(ST_GeomFromWKT(t1.WKT), ST_GeomFromWKT(t2.WKT)) as Within FROM geometry1 t1, geometry2 t2\");\n",
    "\n",
    "# Check NullGeometry\n",
    "nullGeo = spark.sql(\"SELECT ST_IsNullGeom(ST_GeomFromWKT(t1.WKT)) as IsNullGeometry FROM geometry1 t1\");\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f4590b19-3140-4e7f-93e7-d023e72b5596",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## **Measurement Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d06a3ee4-ca64-4237-b8de-e6cd86a3fd8d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Area\n",
    "getAreaInMile = spark.sql(\"SELECT ST_Area(ST_GeomFromWKT(WKT), 'sq mi', 'SPHERICAL') as Calculated_Area_Mile FROM polygontest\")\n",
    "getAreaInMile.show(2)\n",
    "getAreaInKM = spark.sql(\"SELECT ST_Area(ST_GeomFromWKT(WKT), 'sq km', 'SPHERICAL') as Calculated_Area_KM FROM polygontest\")\n",
    "\n",
    "# Distance\n",
    "getDistance = spark.sql(\"SELECT ST_Distance(ST_GeomFromWKT(t1.WKT), ST_GeomFromWKT(t2.WKT), 'm', 'SPHERICAL') FROM geometry1 t1, geometry2 t2\")\n",
    "\n",
    "# Length \n",
    "getLengthInKM = spark.sql(\"SELECT ST_Length(ST_GeomFromWKT(WKT), 'm', 'CARTESIAN') as Calculated_Length_KM FROM polygontest\")\n",
    "\n",
    "# Perimeter\n",
    "getPerimeter = spark.sql(\"SELECT ST_Perimeter(ST_GeomFromWKT(WKT), 'km', 'SPHERICAL') as Calculated_Perimeter_KM FROM polygontest\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1be1d9eb-6eec-4dcd-b153-6b5510e65c68",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## **Processing Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a82b615-397c-4972-a3d4-3c074a77271d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Buffer\n",
    "getBuffer = spark.sql(\"SELECT ST_Buffer(ST_GeomFromWKT(WKT), 5.0 , 'km', 4, 'SPHERICAL') as Calculated_Buffer_KM FROM polygontest\")\n",
    "\n",
    "# convex hull\n",
    "convexhull = spark.sql(\"SELECT ST_ConvexHull(ST_GeomFromWKT(WKT)) as Convex_Hull FROM polygontest\")\n",
    "\n",
    "# Intersection\n",
    "intersection = spark.sql(\"SELECT ST_Intersection(ST_GeomFromWKT(t1.WKT), ST_GeomFromWKT(t2.WKT)) as Intersection FROM geometry1 t1, geometry2 t2\");\n",
    "\n",
    "# Transform\n",
    "tranform = spark.sql(\"SELECT ST_Transform(ST_GeomFromWKT(WKT), 'epsg:3857') as Tranform FROM polygontest\")\n",
    "\n",
    "# union\n",
    "union = spark.sql(\"SELECT ST_Intersection(ST_GeomFromWKT(t1.WKT), ST_GeomFromWKT(t2.WKT)) as Unions FROM geometry1 t1, geometry2 t2\");\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "edcddc12-928d-477a-80ae-3cc1d575276e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## **Observer Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e8ca3292-584c-4443-813b-0ca7c89c8701",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ST_X\n",
    "ST_X = spark.sql(\"SELECT ST_X(ST_Point(-73.750333 , 42.736103)) as X_Cordinate\")\n",
    "\n",
    "# ST_XMax\n",
    "ST_XMax = spark.sql(\"SELECT  ST_XMax(ST_Point(33.750333 , 42.736103)) as X_Max_Cordinate\")\n",
    "\n",
    "# ST_Xmin\n",
    "ST_XMin = spark.sql(\"SELECT  ST_XMin(ST_Point(33.750333 , 42.736103)) as X_Min_Cordinate\")\n",
    "\n",
    "# ST_Y\n",
    "ST_Y = spark.sql(\"SELECT ST_Y(ST_Point(-73.750333 , 42.736103)) as Y_Cordinate\")\n",
    "\n",
    "# ST_YMax\n",
    "ST_YMax = spark.sql(\"SELECT  ST_YMax(ST_Point(33.750333 , 42.736103)) as Y_Max_Cordinate\")\n",
    "\n",
    "# ST_Ymin\n",
    "ST_YMin = spark.sql(\"SELECT  ST_YMin(ST_Point(33.750333 , 42.736103)) as Y_Min_Cordinate\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "da9c9203-2385-4bd8-bad8-30024a73cd88",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## **Grid Function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "33eaf618-2a1e-43d3-9f26-d4ae500216c3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# GeohashId\n",
    "GeoHashId = spark.sql(\"SELECT ST_GeoHash(ST_Point(-73.750333, 42.736103),3) as GeoHashId\")\n",
    "\n",
    "# GeoHashBoundary\n",
    "GeoHashBoundary = spark.sql(\"SELECT ST_GeoHashBoundary('dre') as GeoHashBoundary\")\n",
    "\n",
    "# HexHash\n",
    "HexHash = spark.sql(\"SELECT ST_HexHash(ST_Point(-73.750333, 42.736103),3) as HexHash\")\n",
    "\n",
    "# HexagonBoundary\n",
    "HexagonBoundary = spark.sql(\"SELECT ST_HexHashBoundary('PF704') as HexagonBoundary\")\n",
    "\n",
    "# ST_SquareHash\n",
    "SquareHash = spark.sql(\"SELECT ST_SquareHash(ST_Point(-73.750333, 42.736103),3) as SquareHash\")\n",
    "\n",
    "# GeoHashBoundary\n",
    "SquareHashBoundary = spark.sql(\"SELECT ST_SquareHashBoundary('030') as SquareHashBoundary\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3d7d7ab2-ca20-49d9-b1ba-3d80664bad7b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "UDF-Pyspark",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
