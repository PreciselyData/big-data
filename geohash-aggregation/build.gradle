apply plugin: 'scala'

version = '1.0.0'
// Check cluster or spark environment for appropriate scala version
//def scala_version = '2.11'
def scala_version = '2.12'
def spark_version= '3.0.2'

repositories {
	mavenCentral()
}

wrapper {
	gradleVersion = "6.4.1"
}

dependencies {
	compileOnly "org.apache.spark:spark-core_$scala_version:$spark_version"
	compileOnly "org.apache.spark:spark-sql_$scala_version:$spark_version"

	implementation fileTree(dir: "lib", include: "*.jar")

	testImplementation "org.apache.spark:spark-core_$scala_version:$spark_version"
	testImplementation "org.apache.spark:spark-sql_$scala_version:$spark_version"
	testImplementation "org.scalatest:scalatest_$scala_version:3.2.2"
	testImplementation "org.scalatest:scalatest-funsuite_$scala_version:3.2.2"
	testImplementation "org.scalatestplus:junit-4-13_$scala_version:3.2.2.0"
	testImplementation "junit:junit:4.13"
}

tasks.withType(Test) {
	//set Hadoop binary lib for windows
	systemProperty "hadoop.home.dir", "${project.rootDir}/hadoop-2.7.1"
}

//task to build jar that can run the verification on a cluster
task testJar(type: Jar) {
	archiveClassifier.set('tests-all')
	from sourceSets.test.output
	//adding scalatest for assertions
	["scalatest-core_${scala_version}", "scalactic_${scala_version}", "scalatest-compatible"].each { lib ->
		from zipTree(configurations.testRuntimeClasspath.filter{ it.name.contains(lib) }.getSingleFile())
	}
}
