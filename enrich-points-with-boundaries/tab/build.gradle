plugins {
	id 'scala'
}

version = '1.0.0'
// Check cluster or spark environment for appropriate scala version
def scala_version = '2.11'
//def scala_version = '2.12'
def spark_version = '2.4.4'

archivesBaseName = "${project.parent.name}-${project.name}_${scala_version}"
dependencies {
	compileOnly "org.apache.spark:spark-core_$scala_version:$spark_version"
	compileOnly "org.apache.spark:spark-sql_$scala_version:$spark_version"

	implementation fileTree(dir: "$rootDir/lib", include: "*.jar")

	testImplementation "org.apache.spark:spark-core_$scala_version:$spark_version"
	testImplementation "org.apache.spark:spark-sql_$scala_version:$spark_version"
	testImplementation "org.scalatest:scalatest_$scala_version:3.2.2"
	testImplementation "org.scalatest:scalatest-funsuite_$scala_version:3.2.2"
}

task scalaTest(dependsOn: ['testClasses'], type: JavaExec) {
	main = 'org.scalatest.tools.Runner'
	args = ['-R', 'build/classes/scala/test', '-o']
	classpath = sourceSets.test.runtimeClasspath
	systemProperty "hadoop.home.dir", "${project.rootDir}/hadoop-2.7.1"
	maxHeapSize = "2560m"
}
test.dependsOn scalaTest

//task to build jar that can run the verification on a cluster
task testJar(type: Jar) {
	archiveClassifier.set('tests-all')
	from sourceSets.test.output
	from zipTree(configurations.testRuntimeClasspath.filter {
		File f -> f.name.contains("scalatest-core_${scala_version}")
	}.getSingleFile()) //adding scalatest for assertions
}